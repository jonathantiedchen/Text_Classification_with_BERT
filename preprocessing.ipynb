{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonlines) (23.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xlrd in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%pip install jsonlines\n",
    "import jsonlines as jl\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install openpyxl\n",
    "%pip install xlrd\n",
    "%pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '1_Data/Raw_Data/train.jsonl'\n",
    "test_path = '1_Data/Raw_Data/test.jsonl'\n",
    "dev_path = '1_Data/Raw_Data/dev.jsonl'\n",
    "\n",
    "def read_jsonl_to_df(filepath):\n",
    "    data = []\n",
    "    with jl.open(filepath) as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "#create DataFrames\n",
    "train_df_old = read_jsonl_to_df(train_path)\n",
    "train_df = read_jsonl_to_df(train_path)\n",
    "\n",
    "test_df = read_jsonl_to_df(test_path)\n",
    "dev_df = read_jsonl_to_df(dev_path)\n",
    "add_df = pd.read_excel('1_Data/Raw_Data/additional_data.xls')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jonathan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jonathan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jonathan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jonathan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "def data_preprocess(text):\n",
    "    #lowercase the text\n",
    "    text = text.lower()     \n",
    "    # (2.2) Tokenize the text into words\n",
    "    tokens = word_tokenize(text)   \n",
    "    new_tokens = []\n",
    "    #remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in tokens:\n",
    "        if ( w not in stopwords.words('english') and w not in string.punctuation ) and not drop_word(w):\n",
    "        #if not drop_word(w):\n",
    "            #lemmatize the words\n",
    "            lemma_token = lemmatizer.lemmatize(w)\n",
    "            new_tokens.append(lemma_token)\n",
    "            \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop words in special cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "special_drop = ['\\\"', '']\n",
    "\n",
    "def drop_word(word):\n",
    "    #regex patterns\n",
    "    single_quote_pattern = r\"^'\"\n",
    "    non_alnum_pattern = r\"^[^a-zA-Z0-9]+$\"\n",
    "    \n",
    "    #check: start with quote, only non-alpha-numeric, or in special_drop\n",
    "    if re.match(single_quote_pattern, word) or re.match(non_alnum_pattern, word) or word in special_drop:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Preprocessed\"] = train_df[\"text\"].apply(data_preprocess)\n",
    "test_df[\"Preprocessed\"] = test_df[\"text\"].apply(data_preprocess)\n",
    "dev_df[\"Preprocessed\"] = dev_df[\"text\"].apply(data_preprocess)\n",
    "\n",
    "train_df.to_csv('1_Data/Raw_Data/train_preprocessed.csv', index=False)\n",
    "train_df_old.to_csv('1_Data/Raw_Data/train_old.csv', index=False)\n",
    "test_df.to_csv('1_Data/Raw_Data/test_preprocessed.csv', index=False)\n",
    "dev_df.to_csv('1_Data/Raw_Data/dev_preprocessed.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'make', 'significant', 'contribution', 'german', 'european', 'hydrogen', 'strategy', 'hence', 'achievement', 'climate', 'target']\n",
      "The project will make a significant contribution to the German and European hydrogen strategy and hence to achievement of the climate targets.\n",
      "['environmentally', 'preferable', 'purchasing', 'guideline', 'formulated', 'enforce', 'green', 'purchasing', 'apply', 'part', 'raw', 'material', 'also', 'packaging', 'material']\n",
      "Our Environmentally Preferable Purchasing Guidelines, which we have formulated to enforce “green” purchasing, apply not only to parts and raw materials but also to packaging materials.\n",
      "['global', 'leader', 'insuring', 'low-carbon', 'technology', 'provide', 'standardized', 'tailor-made', 'insurance', 'product', 'part', 'sustainable', 'solution', 'see', 'section', '03.4']\n",
      "As a global leader in insuring low-carbon technologies, we provide standardized and tailor-made insurance products as part of our Sustainable Solutions (see section 03.4).\n",
      "['bank', 'venture', 'municipal', 'sector', 'also', 'promote', 'environmentally', 'friendly', 'service', 'example', 'improving', 'energy', 'efficiency', 'district', 'heating', 'water', 'treatment', 'facility']\n",
      "Bank ventures in the municipal sector also promote more environmentally friendly services, for example by improving the energy efficiency of district heating or water treatment facilities.\n",
      "['2020', 'continue', 'protect', 'water', 'especially', 'scarce', 'use', 'vital', 'resource', 'harmony', 'business', 'ecosystem', 'local', 'community']\n",
      "Between now and 2020, we will continue to protect water, especially where it is scarce, and use this vital resource in harmony with our business ecosystems and local communities.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data\n",
    "\n",
    "clim = train_df[train_df[\"label\"] == 1].reset_index(drop=True)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(clim[\"Preprocessed\"][i])\n",
    "    print(clim[\"text\"][i])\n",
    "\n",
    "type(train_df[\"Preprocessed\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "text            0\n",
      "label           0\n",
      "Preprocessed    0\n",
      "dtype: int64\n",
      "\n",
      "Test Data:\n",
      "text            0\n",
      "label           0\n",
      "Preprocessed    0\n",
      "dtype: int64\n",
      "\n",
      "Dev Data:\n",
      "text            0\n",
      "label           0\n",
      "Preprocessed    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check for NaN values\n",
    "print(\"Train Data:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "print(\"\\nDev Data:\")\n",
    "print(dev_df.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
